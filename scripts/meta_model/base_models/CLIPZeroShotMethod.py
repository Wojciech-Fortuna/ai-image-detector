from __future__ import annotations

import base64
import io
import math
from typing import Dict, Any, Tuple

import numpy as np
from PIL import Image, ImageOps
import torch
import clip

from .base import BaseMethod, MethodResult, register


AI_PHOTO_PROMPTS = [
    "a photorealistic image generated by an artificial intelligence system",
    "a realistic photograph-like image created automatically by a neural network",
    "a synthetic image of a real object generated by a neural network with slightly strange textures",
    "a realistic looking but artificial image with subtle visual artifacts typical of neural networks",
    "a photo-realistic but synthetic image created by a generative model",
    "an image that looks like a camera photo but was generated by a neural network",
]

REAL_PHOTO_PROMPTS = [
    "a real photograph taken with a physical camera",
    "a real-world photo captured by a camera sensor",
    "a genuine camera photograph of the real world",
    "a candid street photo taken with a handheld camera",
    "a documentary style photograph of real life",
    "a high-resolution photo shot with a DSLR camera",
    "a smartphone photo with natural noise and grain",
    "a real indoor room photo with natural lighting and slight lens imperfections",
    "a real outdoor landscape photo with atmospheric haze",
    "a real portrait photograph of a person with natural skin texture",
]

UNKNOWN_PROMPTS = [
    "a plain gray background with no objects",
    "a simple flat color image with no content",
    "a uniform solid color picture without any shapes or details",
    "a blank image with a single neutral color and nothing else",
    "a very simple texture with almost no visible structure or objects",
    "an empty background with no scene, no people and no objects",
]


def _img_to_b64_png(img: Image.Image) -> str:
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return base64.b64encode(buf.getvalue()).decode("utf-8")


def _prepare_rgb(img: Image.Image) -> Image.Image:
    img = ImageOps.exif_transpose(img)
    if img.mode in ("RGBA", "LA"):
        bg = Image.new("RGB", img.size, (255, 255, 255))
        bg.paste(img, mask=img.split()[-1])
        return bg
    if img.mode != "RGB":
        return img.convert("RGB")
    return img


_CLIP_MODEL = None
_CLIP_PREPROCESS = None
_CLIP_DEVICE = None

_AI_PHOTO_FEATS = None
_REAL_PHOTO_FEATS = None
_UNKNOWN_FEATS = None


def _load_clip_and_text_features_grouped(device: str | None = None):
    global _CLIP_MODEL, _CLIP_PREPROCESS, _CLIP_DEVICE
    global _AI_PHOTO_FEATS, _REAL_PHOTO_FEATS, _UNKNOWN_FEATS

    if _CLIP_MODEL is not None:
        return

    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"

    model, preprocess = clip.load("ViT-B/32", device=device)
    model.eval()

    _CLIP_MODEL = model
    _CLIP_PREPROCESS = preprocess
    _CLIP_DEVICE = device

    texts = AI_PHOTO_PROMPTS + REAL_PHOTO_PROMPTS + UNKNOWN_PROMPTS

    tokens = clip.tokenize(texts).to(device)

    with torch.no_grad():
        feats = model.encode_text(tokens)
        feats = feats / feats.norm(dim=-1, keepdim=True)

    c1 = len(AI_PHOTO_PROMPTS)
    c2 = c1 + len(REAL_PHOTO_PROMPTS)

    _AI_PHOTO_FEATS = feats[:c1]
    _REAL_PHOTO_FEATS = feats[c1:c2]
    _UNKNOWN_FEATS = feats[c2:]


def _compute_probs_3way(
    image_features: torch.Tensor,
    ai_photo_features: torch.Tensor,
    real_photo_features: torch.Tensor,
    unknown_features: torch.Tensor,
    temperature: float = 50.0,
):
    with torch.no_grad():
        sim_ai_photo = image_features @ ai_photo_features.T
        sim_real_photo = image_features @ real_photo_features.T
        sim_unknown = image_features @ unknown_features.T

        max_ai_photo = sim_ai_photo.max(dim=1, keepdim=True).values
        max_real_photo = sim_real_photo.max(dim=1, keepdim=True).values
        max_unknown = sim_unknown.max(dim=1, keepdim=True).values

        logit_ai = max_ai_photo
        logit_nonai = max_real_photo
        logit_unknown = max_unknown

        logits = torch.cat([logit_ai, logit_nonai, logit_unknown], dim=1) * temperature
        p_ai_3, p_nonai_3, p_unknown_3 = logits.softmax(dim=1).cpu().numpy()[0].tolist()

    denom = p_ai_3 + p_nonai_3
    if denom > 0:
        p_ai_bin = p_ai_3 / denom
        p_nonai_bin = p_nonai_3 / denom
    else:
        p_ai_bin = 0.5
        p_nonai_bin = 0.5

    return p_ai_3, p_nonai_3, p_unknown_3, p_ai_bin, p_nonai_bin


def _get_vit_patch_embeddings(image_input: torch.Tensor):
    visual = _CLIP_MODEL.visual

    if image_input.dtype != visual.conv1.weight.dtype:
        image_input = image_input.to(dtype=visual.conv1.weight.dtype)
            
    x = visual.conv1(image_input)
    x = x.reshape(x.shape[0], x.shape[1], -1)
    x = x.permute(0, 2, 1)

    cls = visual.class_embedding.to(x.dtype)
    cls = cls + torch.zeros(x.shape[0], 1, x.shape[2], dtype=x.dtype, device=x.device)
    x = torch.cat([cls, x], dim=1)

    x = x + visual.positional_embedding.to(x.dtype)
    x = visual.ln_pre(x)

    x = x.permute(1, 0, 2)
    x = visual.transformer(x)
    x = x.permute(1, 0, 2)

    x = visual.ln_post(x)
    if hasattr(visual, "proj") and visual.proj is not None:
        x = x @ visual.proj

    tokens = x
    patch = tokens[:, 1:, :]

    n = patch.shape[1]
    side = int(math.sqrt(n))
    if side * side != n:
        H, W = 1, n
    else:
        H = W = side

    norm = patch.norm(dim=-1, keepdim=True) + 1e-8
    patch_norm = patch / norm

    return patch_norm, H, W


def _make_photo_3way_heatmap_overlay(
    img: Image.Image,
    ai_photo_feats: torch.Tensor,
    real_photo_feats: torch.Tensor,
):
    base = _prepare_rgb(img)
    image_input = _CLIP_PREPROCESS(base).unsqueeze(0).to(_CLIP_DEVICE)

    with torch.no_grad():
        patch_emb, H_p, W_p = _get_vit_patch_embeddings(image_input)

    with torch.no_grad():
        ai_cent = ai_photo_feats.mean(dim=0, keepdim=True)
        nonai_cent = real_photo_feats.mean(dim=0, keepdim=True)

        ai_cent /= ai_cent.norm(dim=-1, keepdim=True)
        nonai_cent /= nonai_cent.norm(dim=-1, keepdim=True)

        ai_sim = torch.einsum("nld,md->nlm", patch_emb, ai_cent)[0, :, 0]
        nonai_sim = torch.einsum("nld,md->nlm", patch_emb, nonai_cent)[0, :, 0]
        delta = ai_sim - nonai_sim

    delta_np = delta.cpu().numpy().reshape(H_p, W_p)
    d_min, d_max = float(delta_np.min()), float(delta_np.max())

    if d_max - d_min < 1e-8:
        norm = np.zeros_like(delta_np, dtype=np.float32)
    else:
        norm = (delta_np - d_min) / (d_max - d_min)

    heat_small = (norm * 255).astype(np.uint8)
    heat_img = Image.fromarray(heat_small, "L")
    heat_img = heat_img.resize(base.size, Image.BILINEAR)

    arr = np.array(heat_img, dtype=np.float32) / 255.0
    rgb = np.zeros((arr.shape[0], arr.shape[1], 3), dtype=np.uint8)
    rgb[..., 0] = (arr * 255).astype(np.uint8)
    rgb[..., 2] = ((1 - arr) * 255).astype(np.uint8)

    heat_rgb = Image.fromarray(rgb, "RGB")
    overlay = Image.blend(base.convert("RGB"), heat_rgb, alpha=0.5)

    return overlay, {
        "heat_min": d_min,
        "heat_max": d_max,
        "heat_mean_norm": float(norm.mean()),
        "heat_std_norm": float(norm.std()),
        "heat_h_patches": float(H_p),
        "heat_w_patches": float(W_p),
    }


@register
class CLIPZeroShotMethod(BaseMethod):
    name = "clip_zero_shot"
    description = "Classifies the image as AI-generated or real based on visual similarity using a CLIP model."

    how_title = "CLIP Zero Shot (Imageâ€“Text Similarity)"
    how_text = (
        "This method uses a vision-language model (CLIP) to compare the image against visual patterns "
        "typical of AI-generated images and real photographs. "
        "The result reflects which category the image is more visually similar to."
    )
    
    def analyze(
        self,
        img: Image.Image,
        score_only: bool = False,
        **kwargs: Any,
    ) -> MethodResult:

        device: str | None = kwargs.get("device", None)

        try:
            _load_clip_and_text_features_grouped(device=device)

            base = _prepare_rgb(img)
            image_input = _CLIP_PREPROCESS(base).unsqueeze(0).to(_CLIP_DEVICE)

            with torch.no_grad():
                feats = _CLIP_MODEL.encode_image(image_input)
                feats = feats / feats.norm(dim=-1, keepdim=True)

            p_ai_3, p_nonai_3, p_unknown_3, p_ai_bin, p_nonai_bin = _compute_probs_3way(
                feats,
                _AI_PHOTO_FEATS,
                _REAL_PHOTO_FEATS,
                _UNKNOWN_FEATS,
                temperature=50.0,
            )

            score = float(p_ai_bin)

            if score_only:
                return MethodResult(
                    name=self.name,
                    task="detection",
                    score=score, 
                    metrics={}, 
                    visuals_b64={}
                )

            eps = 1e-12
            entropy_2way = float(
                -(p_ai_bin * math.log(p_ai_bin + eps) + p_nonai_bin * math.log(p_nonai_bin + eps))
            )
            entropy_3way = float(
                -(
                    p_ai_3 * math.log(p_ai_3 + eps)
                    + p_nonai_3 * math.log(p_nonai_3 + eps)
                    + p_unknown_3 * math.log(p_unknown_3 + eps)
                )
            )

            metrics = {
                "p_ai_bin": float(p_ai_bin),
                "p_nonai_bin": float(p_nonai_bin),
                "entropy_2way": entropy_2way,
                "p_ai_3way": float(p_ai_3),
                "p_nonai_3way": float(p_nonai_3),
                "p_unknown_3way": float(p_unknown_3),
                "entropy_3way": entropy_3way,
            }

            try:
                overlay, heat_m = _make_photo_3way_heatmap_overlay(
                    img,
                    _AI_PHOTO_FEATS,
                    _REAL_PHOTO_FEATS,
                )
                heat_b64 = _img_to_b64_png(overlay)
                visuals = {"heatmap": heat_b64}
                metrics.update(heat_m)
            except Exception as e_hm:
                metrics.update({"heat_error": 1.0, "heat_error_msg": str(e_hm)})
                visuals = {}

            return MethodResult(
                name=self.name,
                task="detection",
                score=score,
                metrics=metrics,
                visuals_b64=visuals,
            )

        except Exception as e:
            return MethodResult(
                name=self.name,
                task="detection",
                score=float("nan"),
                metrics={"error": 1.0, "error_msg": str(e)},
                visuals_b64={},
            )
